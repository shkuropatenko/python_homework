Wikipedia robots.txt is basically a set of rules for bots. From the file it’s clear that Wikipedia does not want crawlers to access dynamic or internal parts of the site. For all user agents (User-agent: *), paths like /w/, /api/, /trap/, and various "Special" pages (for example /wiki/Special: and its language-specific or URL-encoded versions) are disallowed. At the same time, there are a few explicit exceptions (Allow) such as mobileview API calls, load.php, REST API documentation, and the sitemap, which are needed for apps and legitimate technical use.

Yes, there are specific rules for certain user agents. Some bots are completely blocked with Disallow: /, including MJ12bot, Mediapartners-Google, and many site-copying tools like HTTrack or WebCopier, as well as wget in recursive mode. For SemrushBot there is a special rule with Crawl-delay: 5, meaning it must slow down its requests. On the other hand, a few bots have no restrictions at all (empty Disallow:), such as IsraBot and Orthogaffe.

Websites use robots.txt to protect their infrastructure and to signal which parts of the site are not meant for automated access. It helps prevent server overload and keeps crawlers away from sensitive or resource-heavy pages. Following robots.txt is part of ethical web scraping: even though it’s not a strict enforcement mechanism, it shows respect for the site and its resources.